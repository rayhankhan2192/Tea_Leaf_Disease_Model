import os
import cv2
import torch
import numpy as np
import logging
from torch.utils.data import Dataset, DataLoader
from typing import Tuple, List, Optional
import albumentations as A
from albumentations.pytorch import ToTensorV2
from sklearn.model_selection import train_test_split

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TeaLeafDataset(Dataset):
    def __init__(
        self,
        data_dir: str,
        transform: Optional[callable] = None,
        subset: str = 'train',
        image_size: Tuple[int, int] = (224, 224),
        class_names: Optional[List[str]] = None
    ):
        self.data_dir = data_dir
        self.transform = transform
        self.subset = subset
        self.image_size = image_size

        # Set class names once
        if class_names is not None:
            self.class_names = class_names  
        else:
            self.class_names = sorted([
                entry for entry in os.listdir(data_dir)
                if os.path.isdir(os.path.join(data_dir, entry))
            ])

        self.class_to_idx = {name: idx for idx, name in enumerate(self.class_names)}
        
        # Only perform heavy disk operations if we are the 'full' dataset
        if subset == 'full':
            self.samples = self.load_samples()
            self.targets = [s[1] for s in self.samples]
            self.class_weights = self.calculate_class_weights()
        else:
            self.samples = []
            self.targets = []
            self.class_weights = None

    def load_samples(self) -> List[Tuple[str, int]]:
        samples = []
        for class_name in self.class_names:
            class_path = os.path.join(self.data_dir, class_name)
            if not os.path.exists(class_path): continue
            for img_name in os.listdir(class_path):
                if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):
                    samples.append((os.path.join(class_path, img_name), self.class_to_idx[class_name]))
        return samples

    def calculate_class_weights(self) -> torch.Tensor:
        class_counts = np.zeros(len(self.class_names), dtype=np.int64)
        for t in self.targets:
            class_counts[t] += 1
        total = len(self.targets)
        weights = [total / (len(self.class_names) * c) if c > 0 else 0.0 for c in class_counts]
        return torch.tensor(weights, dtype=torch.float32)

    def log_summary(self):
        logger.info(f"Dataset Summary: {self.subset}")
        logger.info(f"Total samples: {len(self.samples)}")
        counts = np.bincount(self.targets, minlength=len(self.class_names))
        for name, count in zip(self.class_names, counts):
            logger.info(f"  {name}: {count}")

    def __len__(self) -> int: return len(self.samples)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, int]:
        img_path, label = self.samples[idx]
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image = cv2.resize(image, self.image_size)
        #image = image.astype(np.float32) / 255.0
        if self.transform:
            image = self.transform(image=image)['image']
        return image, label

def get_tea_leaf_transforms(image_size, mode):
    if mode == 'train':
        return A.Compose([
            A.Rotate(limit=15, p=0.7),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.2),
            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
            ToTensorV2()
        ])
    return A.Compose([
        A.Resize(image_size[0], image_size[1]),
        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
        ToTensorV2()
    ])

def create_data_loaders(data_dir, batch_size=32, class_names=None, image_size=(224, 224)):
    # Scan disk ONLY ONCE
    full_ds = TeaLeafDataset(data_dir, subset='full', class_names=class_names, image_size=image_size)
    
    train_idx, temp_idx = train_test_split(range(len(full_ds)), train_size=0.8, stratify=full_ds.targets, random_state=42)
    val_idx, test_idx = train_test_split(temp_idx, train_size=0.5, stratify=[full_ds.targets[i] for i in temp_idx], random_state=42)

    def build_subset(indices, subset, mode):
        ds = TeaLeafDataset(data_dir, transform=get_tea_leaf_transforms(image_size, mode), subset=subset, class_names=full_ds.class_names)
        ds.samples = [full_ds.samples[i] for i in indices]
        ds.targets = [s[1] for s in ds.samples]
        ds.log_summary() # Logs once per split
        return DataLoader(ds, batch_size=batch_size, shuffle=(mode=='train'))

    return build_subset(train_idx, 'train', 'train'), build_subset(val_idx, 'val', 'val'), \
           build_subset(test_idx, 'test', 'test'), full_ds.class_weights